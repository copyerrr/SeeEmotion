# ai_engine/ver2_video_runner.py
# ---------------------------------------------------------
# Video â†’ STT â†’ Emotion â†’ Intensity â†’ BGM/SFX â†’ Caption GUI (ver2)
#   - ffmpeg ë¡œ mp4ì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ
#   - Deepgram ì‹¤ì‹œê°„ STT
#   - ë„ì–´ì“°ê¸° ë³´ì •, ê°ì • ë¶„ì„, ìŒëŸ‰ ê¸°ë°˜ ê°•ë„, BGM/íš¨ê³¼ìŒ íƒœê¹…
#   - ìµœì¢… ê²°ê³¼ëŠ” caption_gui.run_caption_gui ì—ê²Œ ì „ë‹¬
# ---------------------------------------------------------

import os
import sys
import threading
import queue
import subprocess
from pathlib import Path

from deepgram import DeepgramClient, LiveTranscriptionEvents, LiveOptions
from dotenv import load_dotenv

# =====================================================================
# 0) í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œë¥¼ sys.path ì— ì¶”ê°€
#    (DX_project_ai-engine/ ê°€ import ê²€ìƒ‰ ê²½ë¡œì— ë“¤ì–´ê°€ë„ë¡)
# =====================================================================
BASE_DIR = Path(__file__).resolve().parent.parent  # .../DX_project_ai-engine
if str(BASE_DIR) not in sys.path:
    sys.path.insert(0, str(BASE_DIR))

# =====================================================================
# 1) ai_engine ë‚´ë¶€ ëª¨ë“ˆ import
# =====================================================================
from ai_engine.config import STYLE_CONFIG, INTENSITY_FONT_RANGE
from ai_engine.style_palette import PALETTES
from ai_engine.text_spacing import fix_spacing
from ai_engine.audio_intensity import (
    update_energy,
    get_energy,
    intensity_from_energy,
)
from ai_engine.emotion_wrapper import analyze_emotion
# from ai_engine.bgm_analyzer import (
#     analyze_bgm_mood,
#     BGM_TEXT_MAP,
#     SFX_TEXT_MAP,
#     current_bgm_label,
#     current_sfx_label,
# )

from ai_engine.speaker_diarization import get_major_speaker, map_speaker_id, stabilize_speaker
from ai_engine.caption_gui import run_caption_gui
# from ai_engine import panns_bgm_analyzer

# =====================================================================
# 2) Deepgram ì„¤ì •
# =====================================================================
load_dotenv(BASE_DIR / ".env")

DEEPGRAM_API_KEY = os.getenv("DEEPGRAM_API_KEY")
if not DEEPGRAM_API_KEY:
    raise RuntimeError("DEEPGRAM_API_KEY ê°€ .env ì— ì—†ìŠµë‹ˆë‹¤.")

dg = DeepgramClient(DEEPGRAM_API_KEY)
dg_connection = dg.listen.websocket.v("1")

# GUI ìŠ¤ë ˆë“œë¡œ ë©”ì‹œì§€ë¥¼ ë„˜ê¸¸ í
text_queue: "queue.Queue[dict]" = queue.Queue()

# íŒ”ë ˆíŠ¸ ì„ íƒ (1/2/3)
PALETTE_LEVEL = STYLE_CONFIG.get("palette_level", 2)
EMOTION_COLORS = PALETTES[PALETTE_LEVEL]


# =====================================================================
# 3) Deepgram Transcript ì½œë°±
# =====================================================================
def on_message(connection, result, **kwargs):
    """Deepgram ì—ì„œ Transcript ì´ë²¤íŠ¸ê°€ ì˜¬ ë•Œë§ˆë‹¤ í˜¸ì¶œë˜ëŠ” ì½œë°±"""

    # ë§ ì‹œì‘ ì´ë²¤íŠ¸ëŠ” ê·¸ëƒ¥ ë¬´ì‹œ
    if getattr(result, "type", None) == "SpeechStarted":
        return

    alt = result.channel.alternatives[0]
    raw_text = alt.transcript
    if not raw_text:
        return

    # -----------------------------------------------------------------
    # (1) ë„ì–´ì“°ê¸° ë³´ì •
    # -----------------------------------------------------------------
    text = fix_spacing(raw_text)

    # -----------------------------------------------------------------
    # (2) í™”ì ID (speaker diarization)
    # -----------------------------------------------------------------
    # 2-1) í™”ì: í•´ë‹¹ segment ë‚´ì—ì„œ ê°€ì¥ ë§ì´ ë“±ì¥í•œ speaker id
    raw_speaker_id = get_major_speaker(alt)      # ex) 0, 1, 2, ...
    # 2-2) í…ìŠ¤íŠ¸ ê¸¸ì´, ì´ì „ í™”ì ê³ ë ¤í•´ì„œ ì•ˆì •í™”ëœ í™”ì ë²ˆí˜¸ ë¦¬í„´
    mapped_speaker = stabilize_speaker(raw_speaker_id, text)  # ex) 1, 2, 3, ...
    

    # í™”ë©´ì— ë³´ì—¬ì¤„ prefix ì ìš© ì—¬ë¶€
    show_speaker = STYLE_CONFIG.get("show_speaker_prefix", True)

    if show_speaker and mapped_speaker is not None:
        prefix = f"[ì¸ë¬¼{mapped_speaker}]"
    else:
        prefix = ""   # ê¸°ë³¸ì€ ì•ˆ ë³´ì´ê²Œ


    # -----------------------------------------------------------------
    # (3) ê°ì • ë¶„ì„ + íŒ”ë ˆíŠ¸ ê¸°ë°˜ ìƒ‰ìƒ
    #     - emotion_wrapper.analyze_emotion(text, palette_level)
    #     - return: (emotion_label, confidence, hex_color)
    # -----------------------------------------------------------------
    palette_level = STYLE_CONFIG.get("palette_level", 2)
    emotion_on = STYLE_CONFIG.get("emotion_on", True)

    if emotion_on:
        # ê°ì • ê¸°ë°˜ íŒ”ë ˆíŠ¸ ì ìš©
        emotion, conf, color_hex = analyze_emotion(text, palette_level)
    else:
        # ê°ì • ìƒ‰ ëˆ ëª¨ë“œ (ë‹¤í/ë‰´ìŠ¤ìš©)
        emotion = "neutral"
        conf = 1.0
        color_hex = "#FFFFFF"  # âœ… ëˆˆì— í™• ë³´ì´ëŠ” ë…¸ë€ìƒ‰
    

    # -----------------------------------------------------------------
    # (4) ì˜¤ë””ì˜¤ ìŒëŸ‰ ê¸°ë°˜ intensity ê³„ì‚° (0 ~ 1)
    # -----------------------------------------------------------------
    rms = get_energy()
    intensity = intensity_from_energy(rms)

    # # -----------------------------------------------------------------
    # # (5) BGM / íš¨ê³¼ìŒ í…ìŠ¤íŠ¸ (PANNs ê²°ê³¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    # # -----------------------------------------------------------------
    # bgm_text = panns_bgm_analyzer.current_bgm_text
    # sfx_text = panns_bgm_analyzer.current_sfx_text

    # print("[DEBUG BGM TEXT]", bgm_text, sfx_text)
    

    # -----------------------------------------------------------------
    # (6) GUI ë¡œ ë„˜ê¸¸ payload êµ¬ì„±
    # -----------------------------------------------------------------
    cap = {
        "speaker": prefix,
        "emotion": emotion,
        "color": color_hex,
        "text": text,
        "intensity": intensity,
        # "bgm_text": bgm_text,
        # "sfx_text": sfx_text,
    }

    # ë””ë²„ê¹…ìš© ë¡œê·¸
    print(
        "[LIVE]",
        f"{prefix} [{emotion}] {text} "
        f"(conf={conf:.3f}, int={intensity:.2f})",
    )


    # GUI ìŠ¤ë ˆë“œì— ì „ë‹¬
    text_queue.put(cap)


# ì½œë°± ë“±ë¡
dg_connection.on(LiveTranscriptionEvents.Transcript, on_message)


# =====================================================================
# 4) FFmpeg: Video â†’ PCM streaming + ì—ë„ˆì§€/BGM ë¶„ì„
# =====================================================================
VIDEO_PATH = os.path.join("data_samples", "ì¸ì‚¬ì´ë“œì•„ì›ƒ.mp4")


def video_stream():
    """ffmpeg ë¡œ mp4 â†’ 16kHz mono PCM ì„ ë½‘ì•„ì„œ
    - audio_intensity.update_energy
    - bgm_analyzer.analyze_bgm_mood
    - Deepgram ìœ¼ë¡œ ì „ì†¡
    ì„ ë™ì‹œì— ìˆ˜í–‰í•œë‹¤.
    """
    if not os.path.exists(VIDEO_PATH):
        raise FileNotFoundError(f"VIDEO_PATH ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {VIDEO_PATH}")

    cmd = [
        "ffmpeg",
        "-re",
        "-i",
        VIDEO_PATH,
        "-vn",
        "-f",
        "s16le",
        "-acodec",
        "pcm_s16le",
        "-ar",
        "16000",
        "-ac",
        "1",
        "pipe:1",
    ]

    print("ğŸ¬ ffmpeg ì‹œì‘:", " ".join(cmd))

    proc = subprocess.Popen(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.DEVNULL,
        bufsize=4096,
    )

    try:
        while True:
            chunk = proc.stdout.read(4096)
            if not chunk:
                break

            # 1) ê°•ë„(RMS) ì—…ë°ì´íŠ¸
            update_energy(chunk)

            # 2) BGM / SFX ë¶„ì„ (PANNs ê¸°ë°˜) + ì´ë²¤íŠ¸ ìˆ˜ì‹ 
            event = panns_bgm_analyzer.analyze_bgm_chunk(chunk, in_sr=16000)

            if event is not None:
                # BGM/SFX ì „ìš© ìº¡ì…˜ payload
                cap = {"type": "BGM_SFX"}
                # í‚¤ê°€ ìˆì„ ë•Œë§Œ ë„£ê¸° (bgm/sfx ê°ê° ON/OFF ì´ë²¤íŠ¸ í¬í•¨)
                if "bgm_text" in event:
                    cap["bgm_text"] = event["bgm_text"]
                if "sfx_text" in event:
                    cap["sfx_text"] = event["sfx_text"]

                # ë””ë²„ê¹…ìš©
                print("[BGM_SFX_EVENT]", cap)

                text_queue.put(cap)


            # 3) Deepgram ìœ¼ë¡œ ì „ì†¡
            dg_connection.send(chunk)

    finally:
        proc.terminate()
        proc.wait()
        dg_connection.finish()
        print("â›” ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ")


# =====================================================================
# 5) ë©”ì¸ ì‹¤í–‰ë¶€
# =====================================================================
if __name__ == "__main__":
    # 1) Deepgram ì„¸ì…˜ ì‹œì‘
    print("[DEBUG] ver2_video_runner main ì‹œì‘")

    # ì—¬ê¸°ì„œ import í•˜ë©´ ì „ì—­ìœ¼ë¡œ ì¡í˜€ì„œ video_streamì—ì„œë„ ì‚¬ìš© ê°€ëŠ¥
    from ai_engine import panns_bgm_analyzer
    print("[DEBUG] panns_bgm_analyzer import ì™„ë£Œ")

    dg_connection.start(
        LiveOptions(
            model="nova-3",
            language="ko",
            encoding="linear16",
            sample_rate=16000,
            channels=1,
            smart_format=True,
            interim_results=False,
            vad_events=True,
            endpointing= 100,        # ë¬´ìŒ or ë§ ë©ˆì¶¤ ì´í›„ ëª‡ ms ë’¤ë¥¼ í•œ ë¬¸ì¥ì˜ ëìœ¼ë¡œ ë³¼ì§€ ì •í•˜ëŠ” ê°’
            diarize=True,            # í™”ì êµ¬ë¶„ on
            # num_speakers=2
            # utterance_end_ms=1000,  # ì†ì‚­ì„ ê°•í™”
            # vad_turnoff_silence_ms=300,
            # vad_threshold=0.2,     # defaultëŠ” 0.5ì¯¤ / ë‚®ì¶œìˆ˜ë¡ ì‘ì€ ì†Œë¦¬ë„ ì¡í˜
        )
    )

    # 2) ffmpeg ìŠ¤íŠ¸ë¦¬ë° ìŠ¤ë ˆë“œ ì‹œì‘
    worker = threading.Thread(target=video_stream, daemon=True)
    worker.start()

    # 3) ìº¡ì…˜ GUI ì‹¤í–‰ (ë©”ì¸ ìŠ¤ë ˆë“œ)
    run_caption_gui(text_queue, STYLE_CONFIG, INTENSITY_FONT_RANGE, EMOTION_COLORS)
