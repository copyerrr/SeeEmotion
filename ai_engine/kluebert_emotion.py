import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# ---------------------------------------------------------
# 1) KLUE-BERT ê¸°ë°˜ í•œêµ­ì–´ ê°ì • ë¶„ì„ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜´
#    - HuggingFace ëª¨ë¸ í—ˆë¸Œì— ì—…ë¡œë“œëœ fine-tuned ëª¨ë¸
#    - 7ê°€ì§€ ê°ì •(fear, surprise, anger, sadness, neutral, joy, disgust) ë¶„ë¥˜
# ---------------------------------------------------------
MODEL_NAME = "dlckdfuf141/korean-emotion-kluebert-v2"

# ë¬¸ì¥ì„ í† í° IDë¡œ ë³€í™˜í•˜ëŠ” tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# ê°ì • ë¶„ë¥˜ ëª¨ë¸ ìì²´ (KLUE-BERT ê¸°ë°˜)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)

# ---------------------------------------------------------
# 2) ê°ì • ID â†’ ê°ì •ëª… ë§¤í•‘ í…Œì´ë¸”
#    - BERT ëª¨ë¸ì€ 0~6 ìˆ«ìë¥¼ ì¶œë ¥í•˜ë¯€ë¡œ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆê²Œ ë§¤í•‘ í•„ìš”
# ---------------------------------------------------------
ID2EMOTION = {
    0: "fear",      # ê³µí¬
    1: "surprise",  # ë†€ëŒ
    2: "anger",     # ë¶„ë…¸
    3: "sadness",   # ìŠ¬í””
    4: "neutral",   # ì¤‘ë¦½
    5: "joy",       # í–‰ë³µ
    6: "disgust",   # í˜ì˜¤
}

EMOTION_ICON = {
    "fear": "ğŸ˜±",
    "surprise": "ğŸ˜²",
    "anger": "ğŸ˜¡",
    "sadness": "ğŸ˜¢",
    "neutral": "ğŸ™‚",
    "joy": "ğŸ˜Š",
    "disgust": "ğŸ¤¢",
}
 

# ---------------------------------------------------------
# 3) ë©”ì¸ í•¨ìˆ˜: í…ìŠ¤íŠ¸ â†’ (ê°ì •, confidence) ë°˜í™˜
#    - ì…ë ¥ëœ ë¬¸ì¥ì„ í† í°í™” â†’ ëª¨ë¸ ì…ë ¥ â†’ softmax í™•ë¥  ê³„ì‚°
#    - ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ê°ì •ì„ ì˜ˆì¸¡í•˜ì—¬ ë°˜í™˜
# ---------------------------------------------------------
def kluebert_emotion(text: str):
    """ë¬¸ì¥ì„ ì…ë ¥í•˜ë©´ (ê°ì •ì´ë¦„, confidence í™•ë¥ ) í˜•íƒœë¡œ ë°˜í™˜"""

    # ë¹ˆ ë¬¸ìì—´(ex: "   ")ì´ ë“¤ì–´ì˜¤ë©´ ê°ì • ë¶„ì„í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì¤‘ë¦½ ë°˜í™˜
    if not text.strip():
        return "neutral", 0.0

    # ë¬¸ì¥ì„ í† í°í™”í•˜ì—¬ BERT ëª¨ë¸ ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜ (PyTorch tensor)
    inputs = tokenizer(text, return_tensors="pt", truncation=True)

    # ëª¨ë¸ ì¶”ë¡ : gradient ê³„ì‚°ì„ í•˜ì§€ ì•ŠìŒ (ì†ë„â†‘, ë©”ëª¨ë¦¬ ì‚¬ìš©â†“)
    with torch.no_grad():
        logits = model(**inputs).logits       # (1, 7) í˜•íƒœì˜ raw scores
        probs = torch.softmax(logits, dim=1)  # softmax â†’ 0~1 í™•ë¥ ê°’ìœ¼ë¡œ ë³€í™˜

        # ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ê°ì • ID ì„ íƒ
        pred_id = torch.argmax(probs, dim=1).item()

        # ì„ íƒëœ ê°ì •ì˜ í™•ì‹  ì •ë„(í™•ë¥ )
        confidence = float(probs[0][pred_id])

    # ìˆ«ì ID â†’ ê°ì •ëª… ë³€í™˜
    emotion = ID2EMOTION.get(pred_id, "neutral")

    # ë””ë²„ê¹…ìš© ë¡œê·¸ ì¶œë ¥
    print(f"[EMO_DEBUG] pred_id={pred_id}, emotion={emotion}, conf={confidence:.3f}")

    # ìµœì¢… ê²°ê³¼ ë°˜í™˜
    return emotion, confidence
