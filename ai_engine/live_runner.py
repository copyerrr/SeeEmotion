# ai_engine/live_runner.py
# ---------------------------------------------------------
# ì‹¤ì‹œê°„ ë§ˆì´í¬ ì…ë ¥ â†’ Deepgram STT â†’ ê°ì •/ê°•ë„/BGM ë¶„ì„ â†’ GUI ìë§‰
# video_runner.py ì™€ ë™ì¼í•œ íŒŒì´í”„ë¼ì¸, ì…ë ¥ë§Œ ë§ˆì´í¬ë¡œ ë°”ë€ ë²„ì „
# ---------------------------------------------------------

import os
import queue
import threading
from pathlib import Path
import sys

import sounddevice as sd
from deepgram import DeepgramClient, LiveTranscriptionEvents, LiveOptions
from dotenv import load_dotenv

# ---------------------------------------------------------
# ğŸ”¥ 0) íŒ¨í‚¤ì§€ ë£¨íŠ¸(BASE_DIR)ë¥¼ sys.path ì— ì¶”ê°€
# ---------------------------------------------------------
BASE_DIR = Path(__file__).resolve().parent.parent
if str(BASE_DIR) not in sys.path:
    sys.path.insert(0, str(BASE_DIR))

# ---------------------------------------------------------
# 1) ai_engine ë‚´ë¶€ ëª¨ë“ˆ import
# ---------------------------------------------------------
from ai_engine.config import STYLE_CONFIG, INTENSITY_FONT_RANGE
from ai_engine.text_spacing import fix_spacing
from ai_engine.audio_intensity import update_energy, get_energy, intensity_from_energy
from ai_engine.emotion_wrapper import analyze_emotion
from ai_engine.style_palette import get_palette
from ai_engine.caption_gui import run_caption_gui
import ai_engine.bgm_analyzer as bgm


# ---------------------------------------------------------
# 2) env ë¡œë”©
# ---------------------------------------------------------

load_dotenv(BASE_DIR / ".env")

DEEPGRAM_API_KEY = os.getenv("DEEPGRAM_API_KEY")
if not DEEPGRAM_API_KEY:
    raise RuntimeError("DEEPGRAM_API_KEY missing")

# Deepgram ì¤€ë¹„(ì´ˆê¸°í™”)
dg = DeepgramClient(DEEPGRAM_API_KEY)
dg_connection = dg.listen.websocket.v("1")

# ìë§‰ ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ëŠ” í
text_queue = queue.Queue()

# ---------------------------------------------------------
# 3) Deepgram STT ì½œë°±
# ---------------------------------------------------------
def on_message(connection, result, **kwargs):

    if getattr(result, "type", None) == "SpeechStarted":
        return

    alt = result.channel.alternatives[0]
    raw_text = alt.transcript
    if not raw_text:
        return

    # 1) ë„ì–´ì“°ê¸° ë³´ì •
    text = fix_spacing(raw_text)

    # 2) í™”ì ID (speaker diarization)
    speaker = None
    if getattr(alt, "words", None):
        last = alt.words[-1]
        speaker = getattr(last, "speaker", None)

    prefix = f"[ì¸ë¬¼{speaker}]" if speaker is not None else "[S?]"

    # 3) ê°ì • ë¶„ì„ + íŒ”ë ˆíŠ¸ ê¸°ë°˜ ìƒ‰ìƒ
    emotion, conf, color = analyze_emotion(text)

    # 4) ì˜¤ë””ì˜¤ ìŒëŸ‰ ê¸°ë°˜ intensity ê³„ì‚° (0 ~ 1)
    rms = get_energy()
    intensity = intensity_from_energy(rms)

    # 5) BGM / sfx ë¼ë²¨ -> í…ìŠ¤íŠ¸ ë§¤í•‘
    bgm_label = bgm.current_bgm_label
    sfx_label = bgm.current_sfx_label

    bgm_text = bgm.BGM_TEXT_MAP.get(bgm_label)
    sfx_text = bgm.SFX_TEXT_MAP.get(sfx_label)

    print("[DEBUG BGM RAW]", bgm_label, sfx_label)
    print("[DEBUG BGM TEXT]", bgm_text, sfx_text)

    # 6) GUI ë¡œ ë„˜ê¸¸ payload êµ¬ì„±
    caption = {
        "speaker": prefix,
        "emotion": emotion,
        "text": text,
        "intensity": intensity,
        "bgm_text": bgm_text,
        "sfx_text": sfx_text,
    }

    print("[LIVE]", f"{prefix} [{emotion}] {text}")

    text_queue.put(caption)


dg_connection.on(LiveTranscriptionEvents.Transcript, on_message)

# ---------------------------------------------------------
# 4) ë§ˆì´í¬ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë° â†’ Deepgram
# ---------------------------------------------------------
def mic_stream():
    """ì‹¤ì‹œê°„ ë§ˆì´í¬ â†’ Deepgram Websocket"""
    def callback(indata, frames, time, status):
        # indata ê°€ CFFI buffer ë¼ -> bytes ë¡œ ë³€í™˜
        chunk = bytes(indata)

        update_energy(chunk) # 1) ì†Œë¦¬ ì„¸ê¸°(RMS) ì—…ë°ì´íŠ¸
        bgm.analyze_bgm_mood(chunk) # 2) BGM / SFX ë¶„ì„
        dg_connection.send(chunk) # 3) Deepgram ìœ¼ë¡œ ì „ì†¡

    print("ğŸ¤ ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘")
    with sd.RawInputStream(samplerate=16000, blocksize=4096,
                           dtype='int16', channels=1, callback=callback):
        threading.Event().wait()  # ë©”ì¸ ìŠ¤ë ˆë“œê°€ ì¢…ë£Œë˜ì§€ ì•Šê²Œ ìœ ì§€

# ---------------------------------------------------------
# 5) ë©”ì¸ ì‹¤í–‰
# ---------------------------------------------------------
if __name__ == "__main__":

    dg_connection.start(
        LiveOptions(
            model="nova-3",
            language="ko",
            encoding="linear16",
            sample_rate=16000,
            channels=1,
            diarize=True,
            vad_events=True,
            interim_results=False,
            smart_format=True,
        )
    )

    worker = threading.Thread(target=mic_stream, daemon=True)
    worker.start()

    run_caption_gui(
        text_queue,
        STYLE_CONFIG,
        INTENSITY_FONT_RANGE,
        get_palette(STYLE_CONFIG["intensity_level"])
    )





#### ğŸ”¥ ì‹¤í–‰ ë°©ë²• ###
# python ai_engine/live_runner.py
# # ë˜ëŠ”
# python -m ai_engine.live_runner : ai_engine íŒ¨í‚¤ì§€ ì•ˆì— live_runner ëª¨ë“ˆì„ ì‹¤í–‰í•´ì¤˜
